\begin{appendices}
\section{Evaluating the performance}
Since analytical results is achievable, we will evaulate our final results using the $R^2$ score,
\begin{align}
    R^2(\bm{y},\tilde{\bm{y}}) = 1 - \frac{\sum_{i=0}^{N-1}(y_i - \hat{y}_i)^2}{\sum_{i=0}^{N-1}(y_i - \bar{\bm{y}})^2}
    \label{eq:r2}
\end{align}
with $\bar{\bm{y}}$ defined as 
\begin{align*}
    \bar{\bm{y}} = \frac{1}{N}\sum^{n-1}_{i=0} y_i
\end{align*}
and the MSE score,
\begin{align}
    \mathrm{MSE} = \frac{1}{N} \sum^{N-1}_{i=0} (y_i - \hat{y}_i)^2,
    \label{eq:mse}
\end{align}
where the $\hat{y}$ is the analytical results, and $y$ is the numerical results.

\section{Neural Net activation functions} \label{app:act-funcs}
\subsection{Sigmoid}
The \textbf{sigmoid} activation function is given as,
\begin{align}
    \sigma_\mathrm{sig} (z) = \frac{1}{1 + \e{-z}}
    \label{eq:sigmoidal-activation}
\end{align}

\subsection{Hyperbolic tangens}
The \textbf{hyperbolic tangens} activation function is given as
\begin{align}
    \sigma_\mathrm{tanh}(z) = \tanh(z)
    \label{eq:act-tanh}
\end{align}

\subsection{Relu}
The \textbf{relu} or rectifier activation is given as,
\begin{align}
    \sigma_\mathrm{relu}(z) = 
    \begin{cases}
        z & \text{if } z \geq 0 \\
        0 & \text{if } z < 0 \\
    \end{cases}
    \label{eq:act-relu}
\end{align}

\subsection{Leaky relu}
The \textbf{leaky relu} or rectifier activation is given as,
\begin{align}
    \sigma_\mathrm{lrelu}(z) = 
    \begin{cases}
        z & \text{if } z > 0 \\
        0.01z & \text{if } z \leq 0 \\
    \end{cases}
    \label{eq:act-leaky-relu}
\end{align}


\end{appendices}