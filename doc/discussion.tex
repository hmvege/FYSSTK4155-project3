\section{Discussion}
\subsection{Deep neural network}
From table \ref{tab:dnn_general_results_gd} and \ref{tab:dnn_general_results_adam} we performed a run with two different optimizers, gradient descent and Adam\cite{kingma_adam:_2014} for $N_\mathrm{e}=10^5$ epochs. The first thing that stands out is that both results for activation functions Leaky Relu and Relu are rather poor. Already, a few of the results have been removed(for $R^2\lesssim 0.8$). For the remaining results, we have in both cases that Sigmoidal activation and $\tanh$ activation clearly outperforms the Relu methods. 

From comparing the gradient descent results and Adam results, we chose to go forward with the Adam optimizer as our optimizer of choice, as the resulting $\max{\varepsilon_\mathrm{abs}}$, $R^2$ and MSE all appeared to be slightly better. A deep investigation was not performed, and is of consideration for the future. From these results, we then selected the layer combinations which appeared the strongest, and moved on to perform a larger and more thorough run with $N_\mathrm{e}=10^6$ epochs.

\subsection{Testing out optimal hyper parameters}
By using a set of hyper parameters as discussed in the previous paragraph, we retrieved the results which can be viewed in table \ref{tab:dnn_optimal_results_adam}. From this, we see that several combinations are more or less on the same order of magnitude in both error and proximity to the analytical results. A choice was made, and a given set of particular fit parameters where chosen. These were the Adam optimizer, $\tanh$ activation function and one layer with 50 neurons\ref{tab:dnn_optimal_params}. We then moved on to perform further comparisons with the finite difference method of Forward Euler and the Neural Network result based of the optimal parameters. 

\subsection{Cost and Error}
The cost and absolute error evolution of the epochs were investigated in the figure \ref{fig:cost-error}. We observe that the cost function is decreasing with a monotonic lower bound, although it fluctuates within. The same cannot be said for the maximum difference, as we can see et keeps oscillating between an maximum absolute error of $10^{-3}$ and $10^{-5}$. The behavior of this is not well understood, and it would be interesting to investigate this with different type of optimizers. The behavior of the Adam optimizer may be easily understood considering it is a stochastic optimizer\footnote{See \href{https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/}{this page} for a discussing surrounding the Adam optimizer.}. The main point to make from \ref{fig:cost-error} is that one may leave the network in a state where is is at somewhat unbound upper error, and not a lower bound error. In fact, judging from the fluctuations, it is likely that many of the other hyper parameters are just as good as the one chosen in table \ref{tab:dnn_optimal_params} or better.

\subsection{Comparing DNN and Forward-Euler against the analytic solution}
In figure \ref{fig:analytical-solution} we presented the analytical solution as given for $N_t=10$ and $N_x=10$ points. With this figure in mind, we can take a look at the figure for the Forward-Euler results \ref{fig:3d-fw-comparison-plots}. Due to the stability condition of Forward-Euler $\alpha = \frac{\Delta t}{\Delta x^2} \leq 1/2$, we have $N_t=100$. This makes direct comparisons between Forward-Euler and the Deep Neural Net somewhat tricky, but we can still draw out some of the main results. By comparing with figure \ref{fig:3d-dnn-comparison-plots}, we can see that the overall absolute error is at least three orders of magnitude greater. This is slightly due to the amount of points we are integrating for along the spatial dimension, and can easily be increased to decrease the error, as see in figure \ref{fig:3d-fw-comparison-plots-spatial-highres}.

Another issue with comparing the results, is that the validity of the results presented are compared against the analytical solution. When solving for a differential equation without any analytical solution, the cost function will be the only available metric to judge the result.


\subsection{Timing}
% While the same metric is not being compared along the $x$-axis in figure \ref{fig:tf-fw-timing-values}, we can see that Forward Euler increases most greatly as a function of the grid we are solving for, while for the Neural Network the most important factor is the layers.
A a closing comment, I would note that the time it takes to run a neural network is far greater than that of finite differences, although that has only briefly been touched upon here in figure \ref{fig:tf-fw-timing-values}. Even though the $x$-axis are not equal in any sense, it is clear that the effectiveness of the results from a DNN is heavily dependent on the number of layers. This is also clear from the optimal parameter table, as that contains the Duration of each run in the righter most column \ref{tab:dnn_optimal_results_adam}. 

Since the accuracy of Forward-Euler is mainly dependent on the number of integration points, the fact that it is by default a quick algorithm provides is with motivation for utilizing it.