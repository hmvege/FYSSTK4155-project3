\section{Results}
\subsection{Deep Neural Net}
We will now proceed with presenting results for a deep neural net using the Gradient Descent optimizer in table \ref{tab:dnn_general_results_gd}, put together using different hyper parameters. Similar hyper parameters but with the Adam optimizer can be viewed in the table \ref{tab:dnn_general_results_adam}. Combinations where put together from different amount of layers, neurons on each layer, activation functions and optimizers. The results were retrieved running $N_\mathrm{e}=10^5$ epochs and having a dropout rate of $\epsilon_\mathrm{Dropout}=0.0$. To evaluate the performance we look at the $R^2$ score\eqref{eq:r2} and the MSE score\eqref{eq:mse}. The $\varepsilon_{\mathrm{abs}}=|u - \hat{u}|$, where $u$ is the numerical results and $\hat{u}$ is the analytical results.
\begin{table}[h!tb]
    \centering
    \caption{Results for a DNN with different hyper parameters. The number of epoch was set to $N_\mathrm{e}=10^5$, and we used a dropout rate of $\epsilon_\mathrm{Dropout}=0.0$. Results presented are obtained with the gradient descent optimizer.}
    \pgfplotstabletypeset[
        every head row/.style={before row=\toprule,after row=\midrule},
        every last row/.style={after row=\bottomrule},
        % columns/optimizer/.style={string type, column name={Optimizer}},
        columns/activation/.style={string type, column name={Activation}},
        columns/layers/.style={column name={Layers}},
        columns/neurons/.style={column name={Neurons}},
        columns/diff/.style={sci, column name=$\mathrm{max}(\varepsilon_{\mathrm{abs}})$},
        columns/r2/.style={column name=$R^2$},
        columns/mse/.style={sci, column name=$MSE$},
        % columns/time/.style={sci, column name=$\Delta t$}
    ]{../results/dnn_general_table_gd.dat}
    \label{tab:dnn_general_results_gd}
\end{table}

\begin{table}[h!tb]
    \centering
    \caption{Results for a DNN with different hyper parameters. The number of epoch was set to $N_\mathrm{e}=10^5$, and we used a dropout rate of $\epsilon_\mathrm{Dropout}=0.0$. Results presented are obtained with the Adam optimizer\cite{kingma_adam:_2014}.}
    \pgfplotstabletypeset[
        every head row/.style={before row=\toprule,after row=\midrule},
        every last row/.style={after row=\bottomrule},
        % columns/optimizer/.style={string type, column name={Optimizer}},
        columns/activation/.style={string type, column name={Activation}},
        columns/layers/.style={column name={Layers}},
        columns/neurons/.style={column name={Neurons}},
        columns/diff/.style={sci, column name=$\mathrm{max}(\varepsilon_{\mathrm{abs}})$},
        columns/r2/.style={column name=$R^2$},
        columns/mse/.style={sci, column name=$MSE$},
        % columns/time/.style={sci, column name=$\Delta t$}
    ]{../results/dnn_general_table_adam.dat}
    \label{tab:dnn_general_results_adam}
\end{table}


% We also run with a dropout rates of $\epsilon_\mathrm{Dropout}=0.25, 0.5$. The results can be viewed in following table
% \begin{table}[h!tb]
%     \centering
%     \caption{Results for a DNN with different hyper parameters. The number of epoch was set to $N_\mathrm{epochs}=10^5$, and we used a dropout rate of $\epsilon_\mathrm{Dropout}=0.25, 0.5$.}
%     \pgfplotstabletypeset[
%         every head row/.style={before row=\toprule,after row=\midrule},
%         every last row/.style={after row=\bottomrule},
%         columns/optimizer/.style={string type, column name={Optimizer}},
%         columns/activation/.style={string type, column name={Activation}},
%         columns/layers/.style={string type, column name={Layers}},
%         columns/diff/.style={sci, column name=$\mathrm{max}(\varepsilon_{\mathrm{abs}})$},
%         columns/r2/.style={column name=$R^2$},
%         columns/mse/.style={sci, column name=$MSE$},
%         columns/dropout/.style={column name=$\epsilon_\mathrm{Dropout}$}
%     ]{../results/dnn_dropout_table.dat}
%     \label{tab:dnn_dropout_results}
% \end{table}

From the table \ref{tab:dnn_general_results_adam} and \ref{tab:dnn_general_results_gd}, the results for which gave us the best $R^2$ and $MSE$ were chosen to be run for $N_\mathrm{e}=10^6$ epochs. The results for this run can be seen in table \ref{tab:dnn_optimal_parameters_results}.
\begin{table}[h!tb]
    \centering
    \caption{Results for a DNN $N_\mathrm{e}10^6$ run with a select set of hyper parameters which yielded the best $R^2$ scores in \ref{tab:dnn_general_results}.}
    \pgfplotstabletypeset[
        every head row/.style={before row=\toprule,after row=\midrule},
        every last row/.style={after row=\bottomrule},
        columns/optimizer/.style={string type, column name={Optimizer}},
        columns/activation/.style={string type, column name={Activation}},
        columns/layers/.style={string type, column name={Layers}},
        columns/diff/.style={sci, column name=$\mathrm{max}(\varepsilon_{\mathrm{abs}})$},
        columns/r2/.style={column name=$R^2$},
        columns/mse/.style={sci, column name=$MSE$},
    ]{../results/dnn_dropout_table.dat}
    \label{tab:dnn_optimal_parameters_results}
\end{table}

\subsection{Forward Euler and finite difference}


\subsection{Comparing DNN and Forward-Euler against the analytic solution}
