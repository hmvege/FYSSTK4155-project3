\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}

% For including pgf tables
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{mathpazo}

% For proper referencing in article
\usepackage{hyperref}
\usepackage{url}

% For figures and graphics'n stuff
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{tabularx}
\usepackage{float}

% For proper appendices
\usepackage[toc,page]{appendix}

% Algorithm packages
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% For bold math symbols
\usepackage{bm}
\usepackage{xcolor}

% For customized hlines in tables
\usepackage{ctable}

% For having latex symbols in section titles
\usepackage{epstopdf}

\pgfplotsset{compat=1.16}

\pgfplotstableset{
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule}
}


% For proper citations
% \usepackage[round, authoryear]{natbib}
\usepackage[numbers]{natbib}

% For fixing large table height
\usepackage{a4wide}

% Remembrance and checking
\newcommand{\husk}[1]{\color{red} #1 \color{black}}

\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\expect}[1]{\langle #1 \rangle} % Proper expectation value
\newcommand{\e}[1]{\mathrm{e}^{#1}} % Proper exponential

% \title{Extracting exited states with machine learning in Lattice QCD}
\title{A Machine learning approach to partial differential equations}
\author{Hans Mathias Mamen Vege}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
A comparison between Forward Euler and Deep Neural Networks, more specifically as multilayer perceptron has been made. From investigating many different combinations of hyper parameters and two different optimizers as seen in table \ref{tab:dnn_general_results_gd} and \ref{tab:dnn_general_results_adam}, we selected the most promising results which we saw in table \ref{tab:dnn_optimal_results_adam}. 

We immediately saw that Leaky Relu and Relu performed poorly when put up against Sigmoidal and $\tanh$ activation, and decided to drop those in any further examinations.

While many of the results found where highly promising, we decided to move on with using the Adam optimizer, one hidden layer with 50 neurons and the $\tanh$ activation function. From this we managed to have the maximum difference drop to $\max(\varepsilon_\mathrm{abs})=4.35\cdot 10^{-5}$, achieve a $R^2 \approx 1.0$ and MSE score of $7.27\cdot 10^{-11}$ and get a cost of $\mathcal{C} = 6.74 \cdot 10^{-7}$. The result of this can be seen in figure \ref{fig:3d-dnn-comparison-plots}, with the absolute error can be seen in \ref{fig:dnn-ana-diff-3d}.

We also investigated the time for different single hidden layer sizes in \ref{fig:tf-timing} and the time dependence on the number of integration points in the temporal direction $N_t$ in figure \ref{fig:fw-timing}. What we found was that already for a simple single hidden layer with 10 neurons and $10^5$ epochs, we use around 15 seconds, with this climbing steeply to well above 1000 seconds for 1000 neurons. For Forward-Euler the accuracy is mainly dependent on the number of integration points, while for the DNN we are heavily reliant on several factors such as layer size and number of epochs. This makes a direct time comparison difficult, but from what we can see the major drawback with a DNN is the time it takes to train.

Taking time into consideration, the poor performance of Forward-Euler in figure \ref{fig:3d-fw-comparison-plots} and for a higher number of integration points in figure \ref{fig:3d-fw-comparison-plots-spatial-highres} is slightly misleading. While the results for the Neural Net is impressive considering very little work is need to set up such a solver, it's time usage is a drawback.

\end{abstract}

% \tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{introduction.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{theory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{implementation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{results.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{discussion.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{conclusion.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{appendix.tex}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}
\bibliography{bibliography/lib,bibliography/comp2015,bibliography/ann_ode_pde,bibliography/tensorflow,bibliography/adam_optimizer}


\end{document}
