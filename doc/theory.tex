\section{Theory}
% A large part of Lattice QCD revolves around the attempt to in trying to extract states, particularly exited states. Problems with this is that they are highly correlated and can be difficult to isolate. Techniques to remedy this exists, in particular the variational method\husk{cite here!!}. We will present an alternate method using \husk{what method?}. But in order to begin, let us first begin by looking at what an exited state is and how a novel quantum mechanical approach can be used to illustrate the problem.

% \subsection{Quantum mechanical exited states}
% Let us begin with looking at a toy example, namely the path integral as formulated in quantum mechanics. Consider quantum mechanics in one dimension where we start at an initial position eigenstate $\ket{x_i}$ at time $t_i$ and end up at a final position eigenstate $\bra{x_f}$ at time $i_f$,
% \begin{align}
%     \bra{x_f} \e{-\hat{H}(t_f - t_i)}\ket{x_i} = \int \mathcal{D} x(t) \e{-S[x]}.
%     \label{eq:qm-path-integral}
% \end{align}
% The $\mathcal{D}$ is indicating the \textit{path integral}, which means that we are integrating over all possible paths\cite{peskin_introduction_1995,shankar_principles_1994}. Each of these paths are weighted by some (Wick rotated) action $S[x]$, which is a functional of the path.

% Our goal is to extract exited states from this function \eqref{eq:qm-path-integral}. In order to do that, we have to rewrite 



% \subsection{A brief primer on Lattice QCD}

% \subsection{Correlators}

% \subsection{Variational Method}

% \subsection{Extracting states with machine learning}

\subsection{A solution to the heat equation}
The partial differential equation we will be investigating is commonly dubbed the heat equation, and has the following shape
\begin{align}
    \frac{\partial^2 u(x,y)}{\partial x^2} = \frac{\partial u(x,t)}{\partial t}
    \label{eq:pde}
\end{align}
given that $t>0$, $x\in [0,L]$. As a shorthand notation, we can write this system as $u_{xx} = u_{t}$. The initial conditions are given as,
\begin{align}
    u(x,0) = \sin(\pi x / L)
    \label{eq:initial-conds}
\end{align}
for $0 < x < L$ and $L = 1$. The boundary conditions are given as,
\begin{align}
    \begin{split}
        u(0,t) = 0 \\
        u(0,L) = 0
    \end{split}
    \label{eq:boundary-conditions}
\end{align}
To derive an analytical solution for our PDE, we begin by assuming that the variables are separable,
\begin{align*}
    u(x,t) = X(x)T(t).
\end{align*}
Inserting this into the PDE \eqref{eq:pde}, we get
\begin{align*}
    \frac{\partial^2 \left(X(x)T(t)\right)}{\partial x^2} = \frac{\partial \left(X(x)T(t)\right)}{\partial t}.
\end{align*}
This becomes,
\begin{align*}
    \frac{1}{X(x)}\frac{\partial^2 X(x)}{\partial x^2} = \frac{1}{T(t)}\frac{\partial T(t)}{\partial t},
\end{align*}
in which we see that each side does not depend on the other, such that they are equal to a constant. Calling this constant $-k^2$, we can solve each of them separately. We begin with the temporal part,
\begin{align*}
    \frac{1}{T(t)}\frac{\partial T(t)}{\partial t} = -k^2,
\end{align*}
which has the solution
\begin{align*}
    V(t) = a \e{-k^2t}.
\end{align*}
For the spatial part, we get
\begin{align}
    \frac{1}{X(x)}\frac{\partial^2 X(x)}{\partial x^2} = -k^2
    \label{eq:pde-temporal-general}
\end{align}
which we can see has a solution given as\cite{boas_mathematical_2006},
\begin{align}
    X(x) = b\sin(kx).
    \label{eq:pde-spatial-general}
\end{align}
We now need to find $a$, $b$ and $k$. From the initial conditions requirement\eqref{eq:initial-conds}, we get
\begin{align*}
    u(x,t=0) = b\sin(kx) \cdot 1
\end{align*}
Requiring $b=1$ fulfills the initial condition requirement\eqref{eq:initial-conds}. If we then look at the boundary conditions\eqref{eq:boundary-conditions},
\begin{align*}
    u(0,t) &= \sin(k\cdot 0)\e{-k^2t} = 0 \\
    u(L,t) &= \sin(k\cdot L)\e{-k^2t} = 0,
\end{align*}
with the last line giving us that,
\begin{align*}
    kL &= \sin (0) \\
    kL &= n \pi \\
    k &= \frac{n\pi}{L}.
\end{align*}
Summing up, we then have our analytical solution
\begin{align}
    u(x,t) = \sin\left(\frac{n\pi x}{L}\right)\e{-\left(\frac{n\pi}{L}\right)t}
    \label{eq:pde-analytical-solution}
\end{align}
We can further set $n=1$ and $L=1$, as that will match the initial conditions in equation \eqref{eq:initial-conds}.

\subsection{Forward Euler and finite differences}
Using an explicit scheme, namely forward Euler\citep[ch. 10.2.2]{hjorth-jensen_computational_2015}, the basis for this scheme is in how to approximate the derivative. Using the familiar forward formula for the derivative, we can write the right hand side of \eqref{eq:pde} as,
\begin{align*}
    \frac{\partial u(x,t)}{\partial t} &\approx \frac{u(x_i, t_j + \Delta t) - u(x_i, t_j)}{\Delta t} \\
    u_t &= \frac{u_{i,j+1} - u_{ij}}{\Delta t}.
\end{align*}
Rewriting the left hand side of \eqref{eq:pde} using a second derivative approximation, we get
\begin{align*}
\frac{\partial^2 u(x,t)}{\partial x^2} &\approx \frac{u(x_i+\Delta x,t_j) - 2u(x_i,t_j) + u(x_i-\Delta x, t_j)}{\Delta x^2} \\
u_{xx} &= \frac{u_{i+1,j} - 2u_{ij} + u_{i-1,j}}{\Delta x^2}.
\end{align*}
Setting these two numerical approximations equal to each other, we can solve for the next iteration in the time direction.
\begin{align*}
    u_{i,j+1} - u_{ij} = \alpha (u_{i+1,j} - 2 u_{ij} + u_{i-1,j}),
\end{align*}
with $\alpha = \frac{\Delta t}{\Delta x^2}$. Solving for $u_{i,j+1}$, we get
\begin{align}
    u_{i,j+1} = \alpha u_{i+1,j} + (1 - 2 \alpha) u_{ij} + \alpha u_{i-1,j}.
    \label{eq:explicit-forward-euler-discretized}
\end{align}
With a keen eye we can recognize this as a simple iterative matrix equation,
\begin{align}
    V^{j+1} = (I - \alpha D) V^j,
    \label{eq:fw-euler-matrix}
\end{align}
with $I$ simply being the identity matrix and $D$ being a tridiagonal matrix of with $-1$ along the of-diagonals and $2$ at the diagonal.
% \begin{align}
%     D = \begin{pmatrix}
%         2 & -1 & 0 & \cdots & \cdots & 0 \\
%         -1 & 2 & -1 & \cdots & \cdots & 0 \\
%         0 & -1 & 2 & \cdots & \cdots & 0 \\
%         \vdots &  &  & \ddots & & \vdots \\
%         0 & \cdots & \cdots & \cdots & 2 & -1 \\
%         0 & \cdots & \cdots & \cdots & -1 & 2 \\
%     \end{pmatrix}
% \end{align}
The stability condition of Forward Euler is given as $\alpha = \frac{\Delta t}{\Delta x^2} \leq 1/2$. In other words, $\Delta t \leq 1/2\Delta x^2$. We will simply refer to \citet[ch. 10.2.1]{hjorth-jensen_computational_2015} for a derivation of this condition.

\subsection{Deep Neural Networks and PDEs}
A full derivation of DNNs will not be given here\footnote{A \textit{more} complete derivation can be found \href{https://github.com/hmvege/StatML/blob/master/doc/backpropagation.pdf}{here}.}, as we will mainly focus on how to apply DNNs to solving PDEs. The derivation will closely follow the one given by \citet{hein-odenn}. The neural net we will utilize is a \textit{Multilayer Perceptron}. To apply it to solving a PDE, we must begin with defining an expression for our PDE. We write our general PDE as,
\begin{align}
    f\left( x, t, g(x, t), g'(x, t), \dots, g^{(n)}(x, t) \right) = 0.
    \label{eq:pde-general}
\end{align}
We have $f$ given to use as some function of $x$, $g(x, t)$ and its derivatives. Without specifying the boundary conditions and initial conditions, this function may have several solutions\cite{boas_mathematical_2006}. The general trial solution(the solution which we will use to approximate $g(x, t)$ with), is given as
\begin{align}
    g_t(x,t) = h_1(x,t) + h_2 (x, t, N(x, t, P)).
    \label{eq:pde-general-trial-solution}
\end{align}
$N(x, t, P)$ is the output from the neural network and $P$ is the weights and biases of the network. As we can see \eqref{eq:pde-general-trial-solution} is written as a sum of two parts, $h_1(x, t)$ and $h_2(x, t, N(x, t, P))$, where $h_1(x, t)$ ensures that $g_t(x, t)$ satisfies a set of boundary conditions\cite{lagaris_artificial_1998}. The boundary conditions \eqref{eq:boundary-conditions} and the initial conditions \eqref{eq:initial-conds}, allows us to make a guess for the $h_1(x, t)$ and $h_2(x, t, N(x, t, P))$.

Having $h_1(x, t)$ as,
\begin{align}
    h_1 (x, t) = (1 - t) \sin (\pi x / L)
    \label{eq:h1}
\end{align}
which at $t=0$ gives $h_1(x,0) = \sin(\pi x / L)$ which satisfies \eqref{eq:initial-conds}. And for $x=0, L$ we get $h_1(x,0)=h_1(x,L)=0$. We now look towards $h_2$.
\begin{align}
    h_2(x,t,N(x,t,P)) = x (1 - x/L)t N(x, t, P)
    \label{eq:h2}
\end{align}
We see that at $t=0$ this equation is zeros, and will also remain zero at the boundaries. Thus, the trial solution\eqref{eq:pde-general-trial-solution} fulfills our requirements.

All that now remains is a cost function to use in the network. We want the network $N(x,t,P)$ to approximate the left hand side with the right hand side of our PDE \eqref{eq:pde} as best as possible\cite[p. 5]{hein-odenn}, this the minimization of this expression can be used as our cost function. We can write,
\begin{align}
    \underset{P}{\min} \left\{\frac{1}{N,N_t} \sum^N_{i=1} G(x,t,P) \right\}
    \label{eq:pde-cost-function}
\end{align}
with $G(x,t,P)$ being
\begin{align}
    G(x, t, P) = \frac{\partial^2 u(x,y)}{\partial x^2} - \frac{\partial u(x,t)}{\partial t} 
\end{align}
The gradient of this cost function will be handled symbolically by TensorFlow\cite{tensorflow2015-whitepaper}.

% As a final note, we can observe that the Neural Network has a complexity of $\mathcal{O}(n^2)$.

% \subsubsection{Optimizers}
% \subsubsection{Layers}
% \subsubsection{Activation functions}
